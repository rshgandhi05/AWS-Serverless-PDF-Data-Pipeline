{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93911ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rshga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cloudscraper\n",
    "from bs4 import BeautifulSoup\n",
    "import PyPDF2\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0891a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Accessing https://www.congress.gov/crs-appropriations-status-table/2026...\n",
      "   Connected to website.\n",
      "   Found Link: H. Rept. 119-217\n",
      "2. Downloading PDF from: https://www.congress.gov/119/crpt/hrpt217/CRPT-119hrpt217.pdf\n",
      "   SUCCESS: File saved as 'appropriations-status-table.pdf'\n"
     ]
    }
   ],
   "source": [
    "# Reading the target url to find the pdf report and Downloading the target report once found \n",
    "target_url = \"https://www.congress.gov/crs-appropriations-status-table/2026\"\n",
    "# Looking for the number \"119-217\" to match the report, number allows for partial matching, avoiding common errors with spaces and formating changes\n",
    "target_report = \"119-217\" \n",
    "pdf_report = \"appropriations-status-table.pdf\"\n",
    "\n",
    "def download_and_save():\n",
    "    # Initializing Cloudscraper to bypass WAF (Web Application Firewall)\n",
    "    scraper = cloudscraper.create_scraper() \n",
    "    print(f\"1. Accessing {target_url}...\")\n",
    "    \n",
    "    try:\n",
    "        # Getting to the main page\n",
    "        response = scraper.get(target_url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\" Failed to connect. Status code: {response.status_code}\")\n",
    "            return False\n",
    "            \n",
    "        print(\"   Connected to website.\")\n",
    "\n",
    "        # Parsing the HTML to find the PDF link by using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        found_link = None\n",
    "        \n",
    "        # Looking for a link containing \"119-161\"\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            if target_report in link.get_text():\n",
    "                found_link = link['href']\n",
    "                print(f\"   Found Link: {link.get_text().strip()}\")\n",
    "                break\n",
    "        \n",
    "        if not found_link:\n",
    "            print(f\"  Error: Could not find a link with text '{target_report}'\") \n",
    "            return False\n",
    "\n",
    "        # Fix link if it starts with \"/\", prepend the base domain to make it a valid URL\n",
    "        if found_link.startswith(\"/\"):\n",
    "            found_link = \"https://www.congress.gov\" + found_link\n",
    "\n",
    "        # Downloading the actual file\n",
    "        print(f\"2. Downloading PDF from: {found_link}\")\n",
    "        pdf_data = scraper.get(found_link)\n",
    "        \n",
    "        # Saving to disk\n",
    "        with open(pdf_report, 'wb') as f:\n",
    "            f.write(pdf_data.content)\n",
    "            \n",
    "        print(f\"   SUCCESS: File saved as '{pdf_report}'\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Crash: {e}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "332cd0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading appropriations-status-table.pdf...\n",
      "2. Scanning 164 pages for 'International Broadcasting Operations'...\n",
      "   Found 1 match(es) on Page 2\n",
      "   Found 2 match(es) on Page 28\n",
      "   Found 1 match(es) on Page 95\n",
      "   Found 1 match(es) on Page 134\n",
      "   Found 1 match(es) on Page 141\n",
      "\n",
      "3. diagnostics complete. Total Matches Found: 6\n"
     ]
    }
   ],
   "source": [
    "#This step Parse the downloaded PDF and retrieve all instances of the phrase “International Broadcasting Operations” or close variants (e.g., different capitalization, minor word-order changes).\n",
    "# It also outputs each instance by page numners.\n",
    "\n",
    "pdf_reportfile = \"appropriations-status-table.pdf\"\n",
    "search_phrase = \"International Broadcasting Operations\"\n",
    "\n",
    "def clean_text_for_search(text):\n",
    "    \"\"\"\n",
    "    Standardizes text so we can find phrases split by lines.\n",
    "    \"\"\"\n",
    "    if not text: return \"\"\n",
    "    # Fixing hyphenation (Inter-\\nnational -> International)\n",
    "    text = re.sub(r'(\\w)-\\n(\\w)', r'\\1\\2', text)\n",
    "    # Fixing newlines (word\\nword -> word word)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    # Removing extra spaces in between\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def verify_matches():\n",
    "    print(f\"1. Loading {pdf_reportfile}...\")\n",
    "    if not os.path.exists(pdf_reportfile):\n",
    "        print(\" Error: File missing.\")\n",
    "        return\n",
    "    # Compiling regex with IGNORECASE so we match \"International\" or \"international\"\n",
    "    reader = PyPDF2.PdfReader(pdf_reportfile)\n",
    "    search_pattern = re.compile(re.escape(search_phrase), re.IGNORECASE)\n",
    "    \n",
    "    match_count = 0\n",
    "    print(f\"2. Scanning {len(reader.pages)} pages for '{search_phrase}'...\")\n",
    "\n",
    "    for i, page in enumerate(reader.pages):\n",
    "        raw_text = page.extract_text()\n",
    "        if not raw_text: continue\n",
    "\n",
    "        # cleaning the text first before searching\n",
    "        cleaned_text = clean_text_for_search(raw_text)\n",
    "        \n",
    "        # counting matches on this page\n",
    "        matches = list(search_pattern.finditer(cleaned_text))\n",
    "        if matches:\n",
    "            match_count += len(matches)\n",
    "            print(f\"   Found {len(matches)} match(es) on Page {i+1}\")\n",
    "\n",
    "    print(f\"\\n3. diagnostics complete. Total Matches Found: {match_count}\")\n",
    "    # expected matches is 6 from the manual check performed\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    verify_matches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d646577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Processing appropriations_report.pdf...\n",
      "2. Done! Found 6 items.\n",
      "   Results saved to: broadcasting_budget_data.json\n"
     ]
    }
   ],
   "source": [
    "#This step is for Extract 150 words before and 150 words after the match.\n",
    "    #Capture the page number on which the instance appears.\n",
    "    #Identify and extract any numbers within that surrounding text and store them separately (as numerical data, not strings).\n",
    "    # saving them in json format named as \"broadcasting_budget_data.json\"\n",
    "\n",
    "pdf_reportfile= \"appropriations_report.pdf\"\n",
    "search_phrase = \"International Broadcasting Operations\"\n",
    "json_output = \"broadcasting_budget_data.json\"\n",
    "\n",
    "# Number Extraction\n",
    "def extract_numbers(text):\n",
    "    \"\"\"\n",
    "    Parses unstructured text to identify financial figures and years.\n",
    "    Returns a list of floats for numerical analysis.\n",
    "    \"\"\"\n",
    "    # \\$?          -> Optional dollar sign\n",
    "    # \\d{1,3}      -> Leading digits (1-3 digits)\n",
    "    # (?:,\\d{3})* -> Optional thousands groups (e.g., ,000)\n",
    "    # (?:\\.\\d+)?   -> Optional decimal cents (e.g., .50)\n",
    "\n",
    "    pattern = r'\\$?\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?'\n",
    "    matches = re.findall(pattern, text)\n",
    "    valid_numbers = []\n",
    "    for m in matches:\n",
    "        clean = m.replace('$', '').replace(',', '')\n",
    "        try:\n",
    "            valid_numbers.append(float(clean))\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return valid_numbers\n",
    "\n",
    "# Running the search\n",
    "def run_pipeline():\n",
    "    print(f\"1. Processing {pdf_reportfile}...\")\n",
    "    reader = PyPDF2.PdfReader(pdf_reportfile)\n",
    "    results = []\n",
    "    \n",
    "    search_pattern = re.compile(re.escape(search_phrase), re.IGNORECASE)\n",
    "\n",
    "    for i, page in enumerate(reader.pages):\n",
    "        raw_text = page.extract_text()\n",
    "        if not raw_text: continue\n",
    "\n",
    "        # Cleaning text from previous cell\n",
    "        cleaned_text = clean_text_for_search(raw_text)\n",
    "        \n",
    "        # Finding & Extracting\n",
    "        for match in search_pattern.finditer(cleaned_text):\n",
    "            \n",
    "            # Contexting Slicing (150 words)\n",
    "            all_words = cleaned_text.split()\n",
    "            match_start_char = match.start()\n",
    "            \n",
    "            # Estimating word index from char index\n",
    "            words_before_list = cleaned_text[:match_start_char].split()\n",
    "            current_word_idx = len(words_before_list)\n",
    "            \n",
    "            # Calculating window\n",
    "            start = max(0, current_word_idx - 150)\n",
    "            phrase_len = len(search_phrase.split())\n",
    "            end = min(len(all_words), current_word_idx + phrase_len + 150)\n",
    "            \n",
    "            # Building context string\n",
    "            words_before = all_words[start : current_word_idx]\n",
    "            words_after = all_words[current_word_idx + phrase_len : end]\n",
    "            full_context = \" \".join(words_before) + f\" [[{match.group()}]] \" + \" \".join(words_after)\n",
    "            \n",
    "            # Extracting Numbers & Save\n",
    "            entry = {\n",
    "                \"page_number\": i + 1,\n",
    "                \"found_phrase\": match.group(),\n",
    "                \"extracted_numbers\": extract_numbers(full_context),\n",
    "                \"context_text\": full_context\n",
    "            }\n",
    "            results.append(entry)\n",
    "\n",
    "    # saving output\n",
    "    with open(json_output, \"w\", encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "        \n",
    "    print(f\"2. Done! Found {len(results)} items.\")\n",
    "    print(f\"   Results saved to: {json_output}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "220efd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading data from broadcasting_budget_data.json...\n",
      "   Loaded 6 records.\n",
      "2. Loading AI Brain (this takes a moment)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Cleaning text and generating summaries...\n",
      "   Summarized Record 1 (Page 2)\n",
      "   Summarized Record 2 (Page 28)\n",
      "   Summarized Record 3 (Page 28)\n",
      "   Summarized Record 4 (Page 95)\n",
      "   Summarized Record 5 (Page 134)\n",
      "   Summarized Record 6 (Page 141)\n",
      "\n",
      " Final report saved to: broadcasting_budget_with_summary.json\n"
     ]
    }
   ],
   "source": [
    "#This is the bonus step to summarize the context around each instance of the phrase \"International Broadcasting Operations\", specifically whether and how the found numbers affect the budget of that program.\n",
    "json_output = \"broadcasting_budget_data.json\"\n",
    "json_output_aisummary = \"broadcasting_budget_with_summary.json\"\n",
    "\n",
    "# Removing all the dots in between\n",
    "def remove_dot_leaders(text):\n",
    "    \"\"\"\n",
    "    Removes the long rows of dots found in budget tables.\n",
    "    Example: \"Agency .................... $100\" -> \"Agency $100\"\n",
    "    \"\"\"\n",
    "    if not text: return \"\"\n",
    "    # Regex: Find 3 or more dots in a row and replace with a single space\n",
    "    clean_text = re.sub(r'\\.{3,}', ' ', text)\n",
    "    # Cleaning up multiple spaces that might result\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "    return clean_text.strip()\n",
    "\n",
    "def run_ai_analysis():\n",
    "    print(f\"1. Loading data from {json_output}...\")\n",
    "    \n",
    "    if not os.path.exists(json_output):\n",
    "        print(\" Error: Input JSON missing.\")\n",
    "        return\n",
    "\n",
    "    with open(json_output, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    print(f\"   Loaded {len(data)} records.\")\n",
    "\n",
    "    #Loading AI Model\n",
    "    print(\"2. Loading AI Brain (this takes a moment)...\")\n",
    "    try:\n",
    "        summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error loading model: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"3. Cleaning text and generating summaries...\")\n",
    "    \n",
    "    for i, entry in enumerate(data):\n",
    "        raw_context = entry['context_text']\n",
    "        \n",
    "        # Cleanning Steps\n",
    "        # Removing the [[ ]] markers\n",
    "        text_no_markers = raw_context.replace(\"[[\", \"\").replace(\"]]\", \"\")\n",
    "        # Removing the ugly dots (................)\n",
    "        clean_input = remove_dot_leaders(text_no_markers)\n",
    "        \n",
    "        # updating the context in the data so final JSON looks pretty\n",
    "        entry['context_text_clean'] = clean_input \n",
    "        \n",
    "        # Truncating to 1024 chars for the AI\n",
    "        ai_input = clean_input[:1024]\n",
    "        \n",
    "        try:\n",
    "            # Generating Summary\n",
    "            summary_output = summarizer(ai_input, max_length=60, min_length=20, do_sample=False)\n",
    "            summary_text = summary_output[0]['summary_text']\n",
    "            \n",
    "            # Post-process summary (sometimes the AI repeats the dots if they were sneaky)\n",
    "            entry['ai_summary'] = remove_dot_leaders(summary_text)\n",
    "            \n",
    "            print(f\"   Summarized Record {i+1} (Page {entry['page_number']})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Could not summarize Record {i+1}: {e}\")\n",
    "            entry['ai_summary'] = \"Error generating summary.\"\n",
    "\n",
    "    #saving final json report with ai summary\n",
    "    with open(json_output_aisummary, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "        \n",
    "    print(f\"\\n Final report saved to: {json_output_aisummary}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_ai_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0468812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
